{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangyuxi/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Clean Data\n",
    "\n",
    "This section handles the initial preparation of the Titanic dataset, including loading the data and performing essential cleaning and feature engineering. Each step is chosen with reasoning to improve data quality and model performance:\n",
    "\n",
    "1. **Load Data**  \n",
    "\n",
    "2. **Clean Data & Feature Engineering**  \n",
    "   - **Extract Titles**: Titles (e.g., Mr, Mrs, Miss) are extracted from passenger names to capture social status, which correlates with survival. Rare titles are grouped as \"Rare\" to avoid sparse categories.  \n",
    "   \n",
    "   - **Impute Missing Ages**: Age is strongly correlated with `Sex`, `Pclass`, and `Title`. We fill missing values using the median within these groups, with additional fallbacks for robustness. This preserves meaningful patterns rather than using a global median.  \n",
    "   \n",
    "   - **Fill Missing Embarked Values**: Only two missing values exist. They are filled with the mode to maintain consistency without biasing the dataset.  \n",
    "   \n",
    "   - **Cabin Features**: The first letter of the cabin is used to capture deck-level information. Missing cabins are labeled `\"NoCabin\"` to retain the distinction between passengers with and without cabin assignments.  \n",
    "   \n",
    "   - **Fill Fare Values**: Fares recorded as 0 are replaced with the median fare grouped by `CabinDeck` and `Embarked`, because fare is correlated with location and cabin level. This avoids distorting the data with invalid 0 values.  \n",
    "   \n",
    "   - **Family Size**: Created as `SibSp + Parch + 1`. Family presence can influence survival probability, capturing the effect of traveling alone vs. with family.\n",
    "\n",
    "This preprocessing ensures a clean, meaningful dataset ready for modeling while retaining important patterns and relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Extract title\n",
    "    df[\"Title\"] = df[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "    rare_titles = df[\"Title\"].value_counts()[df[\"Title\"].value_counts() < 10].index\n",
    "    df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")\n",
    "\n",
    "    # Fill missing Age using median by Sex, Pclass, Title + 2 fallbacks - 20% of missing, age correalated w/ sex/class/title\n",
    "    df[\"Age\"] = df.groupby([\"Sex\", \"Pclass\",\"Title\"])[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"Age\"] = df.groupby([\"Sex\",\"Pclass\"])[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
    "    df[\"Age\"] = df.groupby([\"Title\"])[\"Age\"].transform(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    # Fill Embarked - only 2 missing value\n",
    "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
    "    \n",
    "    # Cabin â†’ HasCabin\n",
    "    df[\"HasCabin\"] = df[\"Cabin\"].notna().astype(int)\n",
    "    df[\"CabinDeck\"] = df[\"Cabin\"].str[0].fillna(\"NoCabin\")\n",
    "    \n",
    "    # Fill Fare\n",
    "    df[\"Fare\"] = df.groupby([\"CabinDeck\", \"Embarked\"])[\"Fare\"].transform(lambda x: x.mask(x==0, x.median()))\n",
    "\n",
    "    # FamilySize\n",
    "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
    "    df[\"FarePerPerson\"] = df[\"Fare\"] / df[\"FamilySize\"]\n",
    "    \n",
    "    # Age group\n",
    "    bins = [0, 12, 18, 35, 60, 120]\n",
    "    labels = [\"Child\", \"Teen\", \"Adult\", \"MidAge\", \"Senior\"]\n",
    "    df[\"AgeGroup\"] = pd.cut(df[\"Age\"], bins=bins, labels=labels)\n",
    "    \n",
    "    # Interaction\n",
    "    df[\"Sex_Pclass\"] = df[\"Sex\"] + \"_\" + df[\"Pclass\"].astype(str)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_base_models():\n",
    "    lr = LogisticRegression(max_iter=500, C=1.0, class_weight='balanced', solver='liblinear', random_state=42)\n",
    "    rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "    voting = VotingClassifier(\n",
    "        estimators=[(\"lr\", lr), (\"rf\", rf), (\"xgb\", xgb)],\n",
    "        voting=\"soft\"\n",
    "    )\n",
    "    return lr, rf, xgb, voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(numeric_features, categorical_features, model):\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def optimize_voting(df):\n",
    "    numeric_features = ['Age', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "    categorical_features = ['Sex', 'Pclass', 'Embarked', 'Title', 'CabinDeck', 'AgeGroup', 'Sex_Pclass']\n",
    "    \n",
    "    _, xgb, lgbm, voting = build_base_models()\n",
    "    pipeline = get_pipeline(numeric_features, categorical_features, voting)\n",
    "    \n",
    "    param_grid = {\n",
    "        'classifier__xgb__n_estimators': [200, 300, 400],\n",
    "        'classifier__xgb__learning_rate': [0.05, 0.1],\n",
    "        'classifier__xgb__max_depth': [3,4,5],\n",
    "        'classifier__lgbm__n_estimators': [300, 400, 500],\n",
    "        'classifier__lgbm__learning_rate': [0.05, 0.1],\n",
    "        'classifier__lgbm__max_depth': [3,4,5]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    X = df[numeric_features + categorical_features]\n",
    "    y = df['Survived']\n",
    "    \n",
    "    grid.fit(X, y)\n",
    "    print(\"Best CV Accuracy:\", grid.best_score_)\n",
    "    print(\"Best Params:\", grid.best_params_)\n",
    "    \n",
    "    return grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Evaluate & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, model):\n",
    "    pipeline = build_pipeline(model)\n",
    "    pipeline.fit(X, y)\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    preds = model.predict(X)\n",
    "    acc = accuracy_score(y, preds)\n",
    "    return acc\n",
    "\n",
    "def predict(model, test_df):\n",
    "    X_test, _ = get_feature_target(test_df, target=None)\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------\n",
    "# Output\n",
    "# ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9629629629629629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangyuxi/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py:158: UserWarning: [01:31:08] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"train.csv\")\n",
    "df = clean_data(df)\n",
    "\n",
    "numeric_features = ['Age', 'Fare', 'FamilySize', 'FarePerPerson']\n",
    "# numeric_features = ['Age', 'Fare', 'FamilySize']\n",
    "categorical_features = ['Sex', 'Pclass', 'Embarked', 'Title', 'CabinDeck', 'AgeGroup', 'Sex_Pclass']\n",
    "# categorical_features = ['Sex', 'Pclass', 'Embarked', 'CabinDeck']\n",
    "\n",
    "X = df[numeric_features + categorical_features]\n",
    "y = df['Survived']\n",
    "\n",
    "lr, xgb, lgbm, voting = build_base_models()  \n",
    "pipeline = get_pipeline(numeric_features, categorical_features, voting)\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    return model.score(X, y)\n",
    "\n",
    "print(\"Train accuracy:\", evaluate(pipeline, X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to submission.csv\n"
     ]
    }
   ],
   "source": [
    "test_df = load_data(\"test.csv\")\n",
    "test_df = clean_data(test_df)\n",
    "\n",
    "X_test = test_df[numeric_features + categorical_features]\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"PassengerId\": test_df[\"PassengerId\"],\n",
    "    \"Survived\": y_pred\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved predictions to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
